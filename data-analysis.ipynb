{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import f_oneway, normaltest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "# make sure that you have downloaded the data from the links provided in the README.md file and kept it in the data folder\n",
    "# load the data\n",
    "\n",
    "parking_tickets_former = 'data/parking-tickets-2017-2019.csv'\n",
    "parking_tickets_later = 'data/parking-tickets.csv'\n",
    "geo_file_path = 'data/geo-locations.csv'\n",
    "local_area_boundary_path = 'data/local-area-boundary.csv'\n",
    "downtown_csvs = 'data/downtown/csvs/downtown_parking_tickets'\n",
    "downtown_combined_heatmap = 'data/downtown/results/downtown_heatmap_all_years.html'\n",
    "dbscan_combined_heatmap = 'data/downtown/results/dbscan_heatmap_all_years.html'\n",
    "\n",
    "# adding the google api key directly for project scope\n",
    "api_key = \"AIzaSyB-MReIlgf2iL04DIAGVc_PZgTDZk9c8aQ\"\n",
    "base_url = \"https://maps.googleapis.com/maps/api/geocode/json?\"\n",
    "\n",
    "vancouver_corrds = [49.2827, -123.1207]\n",
    "\n",
    "data_former = pd.read_csv(parking_tickets_former, sep=';', low_memory=False)\n",
    "data_later = pd.read_csv(parking_tickets_later, sep=';', low_memory=False)\n",
    "\n",
    "data_later.drop('BI_ID', axis=1, inplace=True)\n",
    "\n",
    "# combine the data\n",
    "data = pd.concat([data_former, data_later], axis=0)\n",
    "\n",
    "#remove none useful points\n",
    "data = data[(data['Status'] == 'IS') & ((data['Bylaw'] == 2952) | (data['Bylaw'] == 2849 ))]\n",
    "\n",
    "# create the directory if it does not exist\n",
    "output_directory = \"data/downtown/results/yearly_heatmaps\"\n",
    "csv_directory = \"data/downtown/csvs\"\n",
    "\n",
    "# Create the directory, including any necessary intermediate directories\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.makedirs(csv_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning, Formatting and Preprocessing\n",
    "\n",
    "# create a full address column for geocoding\n",
    "data['full_address'] = data['Block'].astype(str) + ' ' + data['Street'] + ', Vancouver, BC, Canada'\n",
    "\n",
    "if os.path.exists(geo_file_path):\n",
    "    geo_locations = pd.read_csv(geo_file_path)\n",
    "else:\n",
    "    geo_locations = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing: Geocoding the addresses\n",
    "\n",
    "if geo_locations is None:\n",
    "    # get unique addresses for geocoding\n",
    "    unique_addresses = data['full_address'].unique()\n",
    "    \n",
    "    # dictionary to store geocoded addresses\n",
    "    geocode_dict = {\n",
    "        'full_address': [],\n",
    "        'latitude': [],\n",
    "        'longitude': []\n",
    "    }\n",
    "    \n",
    "    # function to geocode address\n",
    "    def geocode_address(address, api_key):\n",
    "        params = {\n",
    "            \"address\": address,\n",
    "            \"key\": api_key\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        geo_data = response.json()\n",
    "        if geo_data['status'] == 'OK':\n",
    "            location = geo_data['results'][0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    # geocode each unique address\n",
    "    for address in unique_addresses:\n",
    "        lat, lng = geocode_address(address, api_key)\n",
    "        geocode_dict['full_address'].append(address)\n",
    "        geocode_dict['latitude'].append(lat)\n",
    "        geocode_dict['longitude'].append(lng)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # create a DataFrame from the geocode_dict\n",
    "    geo_locations = pd.DataFrame(geocode_dict)\n",
    "    \n",
    "    # save the geocoded data to a CSV file\n",
    "    geo_locations.to_csv(geo_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parking_tickets_with_neighbourhood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m data_with_neighborhood\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mcolumns_to_drop, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# save the result to a new CSV file\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m data_with_neighborhood\u001b[38;5;241m.\u001b[39mto_csv(\u001b[43mparking_tickets_with_neighbourhood\u001b[49m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m data \u001b[38;5;241m=\u001b[39m data_with_neighborhood\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parking_tickets_with_neighbourhood' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: Finding the Neighborhoods\n",
    "\n",
    "# rename columns in geo_locations to avoid conflicts during merge\n",
    "geo_locations.rename(columns={'latitude': 'geo_latitude', 'longitude': 'geo_longitude'}, inplace=True)\n",
    "\n",
    "data = data.merge(geo_locations, on='full_address', how='left')\n",
    "\n",
    "# load the local area boundary data\n",
    "local_area_boundary = pd.read_csv(local_area_boundary_path, sep=';')\n",
    "\n",
    "# convert the boundary data to a GeoDataFrame\n",
    "def parse_geom(geom_str):\n",
    "    geom = json.loads(geom_str.replace(\"'\", \"\\\"\"))\n",
    "    return Polygon(geom['coordinates'][0])\n",
    "\n",
    "local_area_boundary['geometry'] = local_area_boundary['Geom'].apply(parse_geom)\n",
    "local_area_boundary_gdf = gpd.GeoDataFrame(local_area_boundary, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# convert the DataFrame to a GeoDataFrame\n",
    "data = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.geo_longitude, data.geo_latitude), crs=\"EPSG:4326\")\n",
    "\n",
    "# ensure the local area boundary data has the same CRS\n",
    "local_area_boundary_gdf = local_area_boundary_gdf.to_crs(data.crs)\n",
    "\n",
    "# perform spatial join and using predicate parameter to ensure that the points are within the polygons\n",
    "data_with_neighborhood = gpd.sjoin(data, local_area_boundary_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# rename the joined column to 'neighborhood'\n",
    "data_with_neighborhood.rename(columns={'Name': 'neighborhood'}, inplace=True)\n",
    "\n",
    "# drop the unnecessary columns\n",
    "columns_to_drop = ['geometry', 'index_right', 'Geom', 'geo_point_2d']\n",
    "data_with_neighborhood.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "data = data_with_neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing: Adding Quarter Information\n",
    "\n",
    "data['EntryDate'] = pd.to_datetime(data['EntryDate'])\n",
    "\n",
    "#get the quarter of \n",
    "def get_quarter(date):\n",
    "    month = date.month\n",
    "    if 1 <= month <= 3:\n",
    "        return 'Q1'\n",
    "    elif 4 <= month <= 6:\n",
    "        return 'Q2'\n",
    "    elif 7 <= month <= 9:\n",
    "        return 'Q3'\n",
    "    elif 10 <= month <= 12:\n",
    "        return 'Q4'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "        \n",
    "data['quarter'] = data['EntryDate'].apply(get_quarter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing: Filtering the Data for Downtown Area\n",
    "\n",
    "#  separate the data for Downtown and West End into new DataFrames\n",
    "downtown_data = data[data['neighborhood'] == 'Downtown']\n",
    "westend_data = data[data['neighborhood'] == 'West End']\n",
    "\n",
    "# reset indices\n",
    "downtown_data = downtown_data.reset_index(drop=True)\n",
    "westend_data = westend_data.reset_index(drop=True)\n",
    "\n",
    "# merge Downtown and West End data as they are both part of the Downtown area\n",
    "downtown_data = pd.concat([downtown_data, westend_data], axis=0)\n",
    "\n",
    "# function to separate the data based on years and create a new CSV file\n",
    "def separate_data_based_on_year(data, file_path):\n",
    "    data.to_csv(f\"{file_path}_{data['Year'].iloc[0]}.csv\", index=False)\n",
    "\n",
    "# apply the function across the grouped data by year\n",
    "grouped_data = downtown_data.groupby('Year', group_keys=False)\n",
    "grouped_data.apply(lambda x: separate_data_based_on_year(x, downtown_csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis: Exploratory Data Analysis\n",
    "\n",
    "# Check data availability\n",
    "data_availability = data.isnull().sum() / len(data) * 100\n",
    "print(\"Data Availability (% of missing values):\")\n",
    "print(data_availability)\n",
    "\n",
    "# Calculate rows per year\n",
    "rows_per_year = data['Year'].value_counts().sort_index()\n",
    "print(\"\\nRows per Year:\")\n",
    "print(rows_per_year)\n",
    "\n",
    "# plot rows per year\n",
    "plt.figure(figsize=(10, 6))\n",
    "rows_per_year.plot(kind='bar')\n",
    "plt.title('Number of Rows per Year')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# calculate rows per neighborhood\n",
    "rows_per_neighborhood = data['neighborhood'].value_counts().sort_values()\n",
    "print(\"\\nRows per Neighborhood:\")\n",
    "print(rows_per_neighborhood)\n",
    "\n",
    "# plot rows per neighborhood\n",
    "plt.figure(figsize=(10, 6))\n",
    "rows_per_neighborhood.plot(kind='barh')\n",
    "plt.title('Number of Rows per Neighborhood')\n",
    "plt.xlabel('Number of Rows')\n",
    "plt.ylabel('Neighborhood')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis: Doing Cluster Analysis on the data\n",
    "\n",
    "years = downtown_data['Year'].unique()\n",
    "file_paths = pd.Series([f'data/downtown/csvs/downtown_parking_tickets_{year}.csv' for year in years], index=years)\n",
    "\n",
    "# function to process each year's data\n",
    "def process_year_data(year):\n",
    "    file_path = file_paths[year]\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # data cleaning\n",
    "    data = data.dropna(subset=['geo_latitude', 'geo_longitude'])\n",
    "    \n",
    "    # base map centered around Vancouver\n",
    "    m = folium.Map(location = vancouver_corrds, zoom_start = 13)\n",
    "    \n",
    "    # data for the heatmap\n",
    "    heat_data = data[['geo_latitude', 'geo_longitude']].values.tolist()\n",
    "    \n",
    "    # calculate density based on the number of infractions\n",
    "    density = len(heat_data)\n",
    "    \n",
    "    # create and add the heatmap\n",
    "    heat_map = HeatMap(heat_data, radius=15, blur=10, max_zoom=1)\n",
    "    m.add_child(heat_map)\n",
    "    \n",
    "    # save the map as an HTML file\n",
    "    m.save(f'data/downtown/results/yearly_heatmaps/downtown_heatmap_{year}.html')\n",
    "    \n",
    "    return density\n",
    "\n",
    "yearly_densities = file_paths.index.to_series().apply(process_year_data)\n",
    "\n",
    "m = folium.Map(location = vancouver_corrds, zoom_start = 13)\n",
    "heat_data = downtown_data[['geo_latitude', 'geo_longitude']].dropna().values.tolist()\n",
    "heat_map = HeatMap(heat_data, radius=15, blur=10, max_zoom=1)\n",
    "m.add_child(heat_map)\n",
    "m.save(downtown_combined_heatmap)\n",
    "\n",
    "# sort the data by year\n",
    "yearly_densities = yearly_densities.sort_index()\n",
    "\n",
    "# plot density changes over years\n",
    "plt.figure(figsize=(10, 6))\n",
    "yearly_densities.plot(kind='bar')\n",
    "plt.title('Density of Infractions per Year')\n",
    "plt.ylabel('Density')\n",
    "plt.xlabel('Year')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis: Doing DBSCAN Clustering on the data\n",
    "\n",
    "# parameters for DBSCAN\n",
    "eps = 0.0003 \n",
    "min_samples = 15 \n",
    "min_count_threshold = 1500  \n",
    "\n",
    "# aggregate data by latitude and longitude\n",
    "combined_data = downtown_data.groupby(['geo_latitude', 'geo_longitude'])['Year'].agg(['count', 'nunique']).reset_index()\n",
    "\n",
    "# apply DBSCAN clustering on aggregated data\n",
    "coords = combined_data[['geo_latitude', 'geo_longitude']].values\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "combined_data['cluster'] = db.labels_\n",
    "\n",
    "persistent_hotspots = combined_data[\n",
    "    (combined_data['nunique'] == len(downtown_data['Year'].unique())) & \n",
    "    (combined_data['count'] >= min_count_threshold) &\n",
    "    (combined_data['cluster'] != -1)\n",
    "]\n",
    "\n",
    "# function to add a marker for each row\n",
    "def add_marker(row):\n",
    "    folium.CircleMarker(\n",
    "        location=(row['geo_latitude'], row['geo_longitude']),\n",
    "        radius=7,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7\n",
    "    ).add_to(m)\n",
    "\n",
    "m = folium.Map(location = vancouver_corrds, zoom_start = 13)\n",
    "persistent_hotspots.apply(add_marker, axis=1)\n",
    "\n",
    "m.save(dbscan_combined_heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis: Doing ANOVA Testing on the data\n",
    "\n",
    "#add month to data\n",
    "data['EntryDate'] = pd.to_datetime(data['EntryDate'])\n",
    "data['month'] = data['EntryDate'].dt.month\n",
    "\n",
    "\n",
    "#separate by years\n",
    "data_2017 = data[(data['Year']==2017)]\n",
    "data_2018 = data[(data['Year']==2018)]\n",
    "data_2019 = data[(data['Year']==2019)]\n",
    "data_2020 = data[(data['Year']==2020)]\n",
    "data_2021 = data[(data['Year']==2021)]\n",
    "data_2022 = data[(data['Year']==2022)]\n",
    "data_2023 = data[(data['Year']==2023)]\n",
    "\n",
    "\n",
    "#group each data by month for each year\n",
    "\n",
    "grouped_data_2017 = data_2017.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2018 = data_2018.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2019 = data_2019.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2020 = data_2020.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2021 = data_2021.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2022 = data_2022.groupby(['month']).size().reset_index(name = 'count')\n",
    "grouped_data_2023 = data_2023.groupby(['month']).size().reset_index(name = 'count')\n",
    "\n",
    "#check for normality before doing anova testing\n",
    "print(f\"2017 normaltest: {normaltest(grouped_data_2017['count']).pvalue}\")\n",
    "print(f\"2018 normaltest: {normaltest(grouped_data_2018['count']).pvalue}\")\n",
    "print(f\"2019 normaltest: {normaltest(grouped_data_2019['count']).pvalue}\")\n",
    "print(f\"2020 normaltest: {normaltest(grouped_data_2020['count']).pvalue}\")\n",
    "print(f\"2021 normaltest: {normaltest(grouped_data_2021['count']).pvalue}\")\n",
    "print(f\"2022 normaltest: {normaltest(grouped_data_2022['count']).pvalue}\")\n",
    "print(f\"2023 normaltest: {normaltest(grouped_data_2023['count']).pvalue}\")\n",
    "# perform anova testing\n",
    "f_statistic, p_value = f_oneway(grouped_data_2017['count'], grouped_data_2018['count'],grouped_data_2019['count'],grouped_data_2020['count'],grouped_data_2021['count'],grouped_data_2022['count'],grouped_data_2023['count'])\n",
    "\n",
    "#\n",
    "print(\"\\nOne-Way ANOVA Results:\")\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P_value: {p_value}\")\n",
    "\n",
    "\n",
    "x_data = pd.DataFrame({'2017':grouped_data_2017['count'], '2018':grouped_data_2018['count'],'2019':grouped_data_2019['count'],'2020':grouped_data_2020['count'],'2021':grouped_data_2021['count'],'2022':grouped_data_2022['count'],'2023':grouped_data_2023['count']})\n",
    "x_melt = pd.melt(x_data)\n",
    "posthoc = pairwise_tukeyhsd(\n",
    "    x_melt['value'], x_melt['variable'],\n",
    "    alpha=0.05)\n",
    "\n",
    "print(posthoc)\n",
    "\n",
    "fig = posthoc.plot_simultaneous()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
